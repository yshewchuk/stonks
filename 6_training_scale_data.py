#!/usr/bin/env python3
"""
Script to scale time windows data for model training.

This script:
1. Loads time windows generated by step 5
2. Scales each window using HistoricalDataScaler
3. Saves the scaled windows to the output directory

Uses multiprocessing for all CPU-intensive operations (reading, scaling, and writing)
to maximize performance and utilize all available CPU cores.
"""

import os
import json
import time
import concurrent.futures
import multiprocessing
from datetime import datetime

import pandas as pd

from config import CONFIG, OUTPUT_DIR, INPUT_DIR
from transforms.historical_data_scaler import HistoricalDataScaler
from utils.dataframe import read_parquet_files_from_directory, write_dataframes_to_parquet
from utils.process import Process

PRICE_COLUMN_TAGS = 'price_column_tags'
VOLUME_PREFIX = 'volume_prefix'
RSI_PREFIX = 'rsi_prefix'
MACD_PREFIX = 'macd_prefix'
MAX_WORKERS = 'max_workers'
# Configuration
CONFIG = CONFIG | {
    INPUT_DIR: "data/5_training_time_windows",
    OUTPUT_DIR: "data/6_training_scale_data",
    PRICE_COLUMN_TAGS: ['Open', 'High', 'Low', 'Close', 'MA', 'Hi', 'Lo'],
    VOLUME_PREFIX: 'Volume',
    RSI_PREFIX: 'RSI',
    MACD_PREFIX: 'MoACD',
    MAX_WORKERS: min(8, max(multiprocessing.cpu_count() - 1, 1)),  # Use CPU count as a guide
    "description": "Scaled time windows data for model training"
}

def _scale_window(args):
    """
    Helper function to scale a single window.
    Used by ProcessPoolExecutor for parallel scaling.
    
    Args:
        args (tuple): Tuple containing (window_name, window_df, price_column_tags, volume_prefix, rsi_prefix, macd_prefix)
        
    Returns:
        tuple: (window_name, scaled_df, error_message)
    """
    window_name, window_df, price_column_tags, volume_prefix, rsi_prefix, macd_prefix = args
    
    try:
        # Create a scaler just for this process
        scaler = HistoricalDataScaler(
            price_column_tags=price_column_tags,
            volume_prefix=volume_prefix,
            rsi_prefix=rsi_prefix,
            macd_prefix=macd_prefix
        )
        
        # Scale the window
        scaled_df = scaler.scale_dataframe(window_df)
        
        return (window_name, scaled_df, None)
    except Exception as e:
        return (window_name, None, str(e))

def main():
    """Main function to run the scaling process."""
    start_time = time.time()
    
    # Initialize the process
    print(f"üöÄ Starting scaling process: {CONFIG[INPUT_DIR]} ‚Üí {CONFIG[OUTPUT_DIR]}")
    Process.start_process(CONFIG)
    
    # Display processor configuration
    print(f"‚ÑπÔ∏è System has {multiprocessing.cpu_count()} CPU cores available")
    print(f"‚ÑπÔ∏è Using up to {CONFIG[MAX_WORKERS]} worker processes for scaling")
    
    # Load time windows from input directory using multiprocessing
    print(f"üîç Loading time windows from {CONFIG[INPUT_DIR]} (multiprocessing)")
    windows_dict = read_parquet_files_from_directory(CONFIG[INPUT_DIR], max_workers=CONFIG[MAX_WORKERS])
    
    if not windows_dict:
        print(f"‚ùå Error: No time windows found in {CONFIG[INPUT_DIR]}")
        return
    
    load_time = time.time()
    print(f"‚úÖ Loaded {len(windows_dict)} time windows in {load_time - start_time:.2f} seconds")
    
    # Scale windows using multiprocessing
    print(f"üîÑ Scaling {len(windows_dict)} time windows (multiprocessing)...")
    scaled_windows = {}
    total_windows = len(windows_dict)
    
    # Prepare tasks for the process pool - don't pass the scaler directly
    # Instead pass the configuration parameters and create a scaler in each process
    tasks = [
        (window_name, window_df, 
         CONFIG[PRICE_COLUMN_TAGS], CONFIG[VOLUME_PREFIX], CONFIG[RSI_PREFIX], CONFIG[MACD_PREFIX]) 
        for window_name, window_df in windows_dict.items()
    ]
    
    # Use ProcessPoolExecutor for true parallel scaling
    # This bypasses the GIL and utilizes multiple CPU cores efficiently
    with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG[MAX_WORKERS]) as executor:
        futures = {executor.submit(_scale_window, task): task[0] for task in tasks}
        
        # Process results as they complete with progress tracking
        completed = 0
        for future in concurrent.futures.as_completed(futures):
            completed += 1
            window_name, scaled_df, error_message = future.result()
            
            if scaled_df is not None:
                scaled_windows[window_name] = scaled_df
                # Print progress every 10 windows or at the end
                if completed % 10 == 0 or completed == total_windows:
                    print(f"  ‚úÖ Progress: {completed}/{total_windows} windows scaled ({completed / total_windows * 100:.1f}%)")
            else:
                print(f"  ‚ùå Error scaling window {window_name}: {error_message}")
    
    scale_time = time.time()
    print(f"‚úÖ Scaled {len(scaled_windows)} windows in {scale_time - load_time:.2f} seconds")
    
    # Save scaled windows using multiprocessing
    print(f"üíæ Saving scaled windows to {CONFIG[OUTPUT_DIR]} (multiprocessing)...")
    success = write_dataframes_to_parquet(scaled_windows, CONFIG, max_workers=CONFIG[MAX_WORKERS])
    
    save_time = time.time()
    
    if success:
        print(f"‚úÖ Successfully saved {len(scaled_windows)} scaled windows to {CONFIG[OUTPUT_DIR]} in {save_time - scale_time:.2f} seconds")
        
        # Save additional metadata
        metadata_path = os.path.join(CONFIG[OUTPUT_DIR], 'scaling_info.json')
        scaling_info = {
            "price_scaling": "relative to first open price",
            "volume_scaling": "min-max scaling within window",
            "rsi_scaling": "fixed range scaling (0-100)",
            "macd_scaling": "min-max scaling within window",
            "total_windows": len(scaled_windows),
            "workers_used": CONFIG[MAX_WORKERS],
            "cpu_cores_available": multiprocessing.cpu_count(),
            "multiprocessing_used": True,
            "processing_time_seconds": {
                "loading": round(load_time - start_time, 2),
                "scaling": round(scale_time - load_time, 2),
                "saving": round(save_time - scale_time, 2),
                "total": round(save_time - start_time, 2)
            }
        }
        
        with open(metadata_path, 'w') as f:
            json.dump(scaling_info, f, indent=2, default=str)
        
        print(f"‚úÖ Saved scaling information to {metadata_path}")
        print(f"üéâ Total processing time: {save_time - start_time:.2f} seconds")
    else:
        print(f"‚ùå Error: Failed to save scaled windows")

if __name__ == "__main__":
    main() 