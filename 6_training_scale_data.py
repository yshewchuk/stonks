#!/usr/bin/env python3
"""
Script to scale time windows data for model training.

This script:
1. Loads time windows generated by step 5
2. Scales each window using HistoricalDataScaler
3. Saves the scaled windows to the output directory

Uses multiprocessing for all CPU-intensive operations to maximize performance.
"""

import os
import json
import time
import concurrent.futures
import multiprocessing
from datetime import datetime

import pandas as pd

from config import (
    CONFIG, OUTPUT_DIR, INPUT_DIR, MAX_WORKERS,
    PRICE_COLUMN_TAGS, VOLUME_PREFIX, RSI_PREFIX, MACD_PREFIX, SCALING_CONFIG
)
from transforms.historical_data_scaler import HistoricalDataScaler
from utils.dataframe import read_parquet_files_from_directory, write_dataframes_to_parquet
from utils.process import Process

# Configuration - use the shared scaling parameters
CONFIG = CONFIG | SCALING_CONFIG | {
    INPUT_DIR: "data/5_training_time_windows",
    OUTPUT_DIR: "data/6_training_scale_data",
    "description": "Scaled time windows data for model training"
}

def _scale_data(args):
    """
    Helper function to scale a single window.
    Used by ProcessPoolExecutor for parallel scaling.
    
    Args:
        args (tuple): Tuple containing (window_name, window_df, price_column_tags, volume_prefix, rsi_prefix, macd_prefix)
        
    Returns:
        tuple: (window_name, scaled_df, error_message)
    """
    window_name, window_df, price_column_tags, volume_prefix, rsi_prefix, macd_prefix = args
    
    try:
        # Create a scaler just for this process
        scaler = HistoricalDataScaler(
            price_column_tags=price_column_tags,
            volume_prefix=volume_prefix,
            rsi_prefix=rsi_prefix,
            macd_prefix=macd_prefix
        )
        
        # Scale the window
        scaled_df = scaler.scale_dataframe(window_df)
        
        return (window_name, scaled_df, None)
    except Exception as e:
        return (window_name, None, str(e))

def main():
    """Main function to run the training data scaling process."""
    start_time = time.time()
    
    # Initialize the process
    print(f"üöÄ Starting training data scaling: {CONFIG[INPUT_DIR]} ‚Üí {CONFIG[OUTPUT_DIR]}")
    Process.start_process(CONFIG)
    
    # Display processor configuration
    print(f"‚ÑπÔ∏è System has {multiprocessing.cpu_count()} CPU cores available")
    print(f"‚ÑπÔ∏è Using up to {CONFIG[MAX_WORKERS]} worker processes")
    
    # Get scaling parameters from config
    price_column_tags = CONFIG[PRICE_COLUMN_TAGS]
    volume_prefix = CONFIG[VOLUME_PREFIX]
    rsi_prefix = CONFIG[RSI_PREFIX]
    macd_prefix = CONFIG[MACD_PREFIX]
    
    print(f"‚ÑπÔ∏è Using scaling configuration:")
    print(f"  - Price column tags: {price_column_tags}")
    print(f"  - Volume prefix: {volume_prefix}")
    print(f"  - RSI prefix: {rsi_prefix}")
    print(f"  - MACD prefix: {macd_prefix}")
    
    # Load time windows from input directory using multiprocessing
    print(f"üîç Loading time windows from {CONFIG[INPUT_DIR]} (multiprocessing)")
    windows_dict = read_parquet_files_from_directory(CONFIG[INPUT_DIR])
    
    if not windows_dict:
        print(f"‚ùå Error: No time windows found in {CONFIG[INPUT_DIR]}")
        return
    
    load_time = time.time()
    print(f"‚úÖ Loaded {len(windows_dict)} time windows in {load_time - start_time:.2f} seconds")
    
    # Scale windows using multiprocessing
    print(f"üîÑ Scaling {len(windows_dict)} time windows (multiprocessing)...")
    scaled_windows = {}
    total_windows = len(windows_dict)
    
    # Prepare tasks for the process pool
    tasks = [
        (window_name, window_df, 
         price_column_tags, volume_prefix, rsi_prefix, macd_prefix) 
        for window_name, window_df in windows_dict.items()
    ]
    
    # Use ProcessPoolExecutor for parallel scaling
    with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG[MAX_WORKERS]) as executor:
        futures = {executor.submit(_scale_data, task): task[0] for task in tasks}
        
        # Process results as they complete with progress tracking
        completed = 0
        for future in concurrent.futures.as_completed(futures):
            completed += 1
            window_name, scaled_df, error_message = future.result()
            
            if scaled_df is not None:
                scaled_windows[window_name] = scaled_df
                # Print progress every 10 windows or at the end
                if completed % 10 == 0 or completed == total_windows:
                    print(f"  ‚úÖ Progress: {completed}/{total_windows} windows scaled ({completed / total_windows * 100:.1f}%)")
            else:
                print(f"  ‚ùå Error scaling window {window_name}: {error_message}")
    
    scale_time = time.time()
    print(f"‚úÖ Scaled {len(scaled_windows)} windows in {scale_time - load_time:.2f} seconds")
    
    # Save scaled windows using multiprocessing
    print(f"üíæ Saving scaled windows to {CONFIG[OUTPUT_DIR]} (multiprocessing)...")
    success = write_dataframes_to_parquet(scaled_windows, CONFIG)
    
    save_time = time.time()
    
    if success:
        print(f"‚úÖ Successfully saved {len(scaled_windows)} scaled windows to {CONFIG[OUTPUT_DIR]} in {save_time - scale_time:.2f} seconds")
        
        # Save additional metadata
        metadata_path = os.path.join(CONFIG[OUTPUT_DIR], 'scaling_info.json')
        scaling_info = {
            "price_scaling": "relative to first open price",
            "volume_scaling": "min-max scaling within window",
            "rsi_scaling": "fixed range scaling (0-100)",
            "macd_scaling": "min-max scaling within window",
            "total_windows_scaled": len(scaled_windows),
            "original_windows": len(windows_dict),
            "failed_windows": len(windows_dict) - len(scaled_windows),
            "multiprocessing_used": True,
            "workers_used": CONFIG[MAX_WORKERS],
            "cpu_cores_available": multiprocessing.cpu_count(),
            "scaling_configuration": {
                "price_column_tags": price_column_tags,
                "volume_prefix": volume_prefix,
                "rsi_prefix": rsi_prefix,
                "macd_prefix": macd_prefix
            },
            "processing_time_seconds": {
                "loading": round(load_time - start_time, 2),
                "scaling": round(scale_time - load_time, 2),
                "saving": round(save_time - scale_time, 2),
                "total": round(save_time - start_time, 2)
            }
        }
        
        with open(metadata_path, 'w') as f:
            json.dump(scaling_info, f, indent=2, default=str)
        
        print(f"‚úÖ Saved scaling information to {metadata_path}")
        print(f"üéâ Total processing time: {save_time - start_time:.2f} seconds")
    else:
        print(f"‚ùå Error: Failed to save scaled windows")

if __name__ == "__main__":
    main() 